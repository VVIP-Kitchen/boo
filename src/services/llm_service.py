import io
import json
import time
import base64
from openai import OpenAI
from typing import List, Dict, Union, Optional
from utils.logger import logger
from utils.config import OPENROUTER_API_KEY, OPENROUTER_MODEL
from services.tool_calling_service import (
  hackernews_tool, get_top_hn_stories,
  tavily_search_tool, search_web,
  sandbox_tool, run_code
)


class LLMService:
  def __init__(self):
    self.client = OpenAI(
      base_url="https://openrouter.ai/api/v1", api_key=OPENROUTER_API_KEY
    )
    self.model = OPENROUTER_MODEL
    self.available_tools = {
      "get_hackernews_stories": {
        "tool_definition": hackernews_tool,
        "function": get_top_hn_stories,
      },
      "search_web": {"tool_definition": tavily_search_tool, "function": search_web},
      "run_code": {
        "tool_definition": sandbox_tool,
        "function": run_code
      }
    }

  def _to_base64_data_uri(self, image: Union[io.BytesIO, bytes]) -> str:
    image_bytes = image.getvalue() if isinstance(image, io.BytesIO) else image
    base64_str = base64.b64encode(image_bytes).decode("utf-8")
    return f"data:image/jpeg;base64,{base64_str}"

  def _has_vision_content(self, messages):
    try:
      for m in messages or []:
        c = m.get("content")
        if isinstance(c, list):
          for item in c:
            if isinstance(item, dict) and item.get("type") == "image_url":
              return True
    except Exception:
      pass
    return False

  def _execute_tool_call(self, tool_call) -> str:
    function_name = tool_call.function.name

    if function_name not in self.available_tools:
      return json.dumps({"error": f"Unknown function: {function_name}"})

    try:
      arguments = json.loads(tool_call.function.arguments)
      function = self.available_tools[function_name]["function"]
      result = function(**arguments)
      return json.dumps(result)
    except Exception as e:
      return json.dumps({"error": f"Error executing {function_name}: {str(e)}"})

  def chat_completions(
    self,
    prompt: str = None,
    image: Union[io.BytesIO, bytes, str] = None,
    messages: Union[str, List[Dict[str, str]]] = None,
    temperature: float = 0.6,
    max_tokens: int = 512,
    enable_tools: bool = True,
    tools: Optional[List[str]] = None,
  ) -> str:
    try:
      mock_usage = type("Usage", (), {"prompt_tokens": 0, "total_tokens": 0})()

      if image:
        image_url = image if isinstance(image, str) else self._to_base64_data_uri(image)
        content = [
          {"type": "text", "text": prompt or "Describe this image."},
          {"type": "image_url", "image_url": {"url": image_url}},
        ]
        chat_messages = [{"role": "user", "content": content}]
      elif messages:
        chat_messages = (
          [{"role": "user", "content": messages}]
          if isinstance(messages, str)
          else messages
        )
      elif prompt:
        chat_messages = [{"role": "user", "content": prompt}]
      else:
        return "‚ö†Ô∏è No input provided.", mock_usage

      api_params = {
        "model": self.model,
        "messages": chat_messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
      }

      vision_mode = self._has_vision_content(chat_messages)
      if enable_tools and not vision_mode:
        tool_definitions = []
        enabled_tools = tools or list(self.available_tools.keys())

        for tool_name in enabled_tools:
          if tool_name in self.available_tools:
            tool_definitions.append(self.available_tools[tool_name]["tool_definition"])

        if tool_definitions:
          api_params["tools"] = tool_definitions
          api_params["tool_choice"] = "auto"
      else:
        api_params["tool_choice"] = "none"

      response = self.client.chat.completions.create(**api_params)
      message = response.choices[0].message

      if not vision_mode and hasattr(message, "tool_calls") and message.tool_calls:
        for tool_call in message.tool_calls:
          tool_result = self._execute_tool_call(tool_call)

          chat_messages.append(
            {
              "role": "assistant",
              "content": message.content,
              "tool_calls": [
                {
                  "id": tool_call.id,
                  "type": "function",
                  "function": {
                    "name": tool_call.function.name,
                    "arguments": tool_call.function.arguments,
                  },
                }
              ],
            }
          )

          chat_messages.append(
            {"role": "tool", "tool_call_id": tool_call.id, "content": tool_result}
          )

          final_response = self.client.chat.completions.create(
            model=self.model,
            messages=chat_messages,
            max_tokens=max_tokens,
            temperature=temperature,
          )
          return final_response.choices[0].message.content.strip(), final_response.usage
      else:
        return message.content.strip(), response.usage
    except Exception as e:
      ### 429 + Retry time
      if hasattr(e, "response") and getattr(e.response, "status_code", None) == 429:
        headers = getattr(e.response, "headers", {})
        reset_ts = int(headers.get("X-RateLimit-Reset", "0"))
        current_ts = int(time.time())
        wait_sec = max(0, reset_ts - current_ts)

        mins = wait_sec // 60
        secs = wait_sec % 60
        formatted = f"{mins}m {secs}s" if mins else f"{secs}s"

        return (
          f"‚è≥ You've hit the rate limit for this model. Try again in {formatted}.",
          mock_usage,
        )

      ### Catch all
      logger.error(f"Unexpected error in chat_completions: {e}")
      return "üòµ Something went wrong while generating a response.", mock_usage
